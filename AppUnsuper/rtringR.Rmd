---
title: "Exploration de texte sur R"
author: "Mondjehi Roland"
date: "2022-11-20"
output: html_document
---

Nous commençons par des transformations de textes courantes, effectuons diverses explorations de données avec une fréquence de terme (tf) et une fréquence de document inverse (idf) et construisons un modèle de classification supervisé qui apprend la différence entre des textes d'auteurs différents.

# Importation des librairies

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages(c("dplyr", "gutenbergr", "stringr", "tidytext", "tidyr",
                   # "stopwords", "wordcloud", "rsample", "glmnet", 
                   # "doMC", "forcats", "broom", "igraph", "ggraph"))

```


#Importation des données

```{r}
library(dplyr)
library(gutenbergr)
library(stringr)

doyle <- gutenbergr::gutenberg_works(str_detect(author, "Doyle"))
```

```{r}

books <- gutenberg_download(c(30155, 13476), meta_fields = "author")
head(books)

```

```{r}
books <- as_tibble(books) %>% 
         mutate(document = row_number()) %>% 
         select(-gutenberg_id)

```

```{r}
head(books)
```

# Transformation des données

## Tokénisation

```{r}
library(tidytext)

tidy_books <- books %>% 
              unnest_tokens(word, text) %>% 
              group_by(word) %>% 
              filter(n() > 10) %>% 
              ungroup()

head(tidy_books)
       
```

## Mots vides
```{r}
library(stopwords)
library(tibble)

stopword <- as_tibble(stopwords("en"))
stopword <- rename(stopword, word = value)
tb <- anti_join(tidy_books, stopword, by = "word")

```

# Analyse exploratoire des données
## Fréquence des termes (tf)

```{r}
words_count <- count(tb, word,sort = TRUE )
words_count
```


```{r}
library(ggplot2)

g1 <- tb %>%  
       count(author, word, sort = TRUE) %>% 
       filter(n > 100) %>% 
       mutate(word = reorder(word, n))
head(g1)
       
```


```{r}
g1 %>% 
       ggplot(aes(word, n))+
       geom_col(aes(fill = author))+
       xlab(NULL)+
       scale_y_continuous(expand = c(0,0))+
       coord_flip()+
       theme_classic(base_size = 12)+
       labs(fill = "Author", title = "Word frequency", subtitle = "n > 100")+
       theme(plot.title = element_text(lineheight = 0.8, face = "bold"))+
       scale_fill_brewer()
       
```

```{r}
tb %>%
  count(author, word, sort = TRUE) %>%
  group_by(author) %>%
  top_n(20) %>%
  ungroup() %>%
  ggplot(aes(reorder_within(word, n, author), n,
    fill = author)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~author, scales = "free") +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic(base_size = 12) +
  labs(fill= "Author", 
       title="Most frequent words", 
       subtitle="Top 20 words by book",
       x= NULL, 
       y= "Word Count")+
  theme(plot.title = element_text(lineheight=.8, face="bold")) +
  scale_fill_brewer()

```


```{r}
newstopwords <- tibble(word = c("eq", "co", "rc", "ac", "ak", "bn", 
                                   "fig", "file", "cg", "cb", "cm",
                               "ab", "_k", "_k_", "_x"))

tb <- anti_join(tb, newstopwords, by = "word")

```

```{r}
tb %>%
  count(author, word, sort = TRUE) %>%
  group_by(author) %>%
  top_n(20) %>%
  ungroup() %>%
  ggplot(aes(reorder_within(word, n, author), n,
    fill = author)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~author, scales = "free") +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic(base_size = 12) +
  labs(fill= "Author", 
       title="Most frequent words after removing stop words", 
       subtitle="Top 20 words by book",
       x= NULL, 
       y= "Word Count")+
  theme(plot.title = element_text(lineheight=.8, face="bold")) +
  scale_fill_brewer()  
```


```{r}

library(wordcloud)

tb %>% 
       count(word) %>% 
       with(wordcloud(word, n, max.words = 15))

```

```{r}
library(forcats)

plot_tb <- tb %>% 
       count(author, word, sort = TRUE) %>% 
       bind_tf_idf(word, author, n) %>% 
       mutate(word = fct_reorder(word, tf_idf)) %>% 
       mutate(author = factor(author,
                              levels = c("Tesla, Nikola",
                                         "Einstein, Albert")))

plot_tb %>% 
  group_by(author) %>% 
  top_n(15, tf_idf) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = author)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~author, ncol = 2, scales = "free") +
  coord_flip() +
  theme_classic(base_size = 12) +
  labs(fill= "Author", 
       title="Term frequency and inverse document frequency (tf-idf)", 
       subtitle="Top 20 words by book",
       x= NULL, 
       y= "tf-idf") +
  theme(plot.title = element_text(lineheight=.8, face="bold")) +
  scale_fill_brewer()  
       
```

```{r}
tf_idf <- tb %>% 
       count(author, word, sort = TRUE) %>% 
       bind_tf_idf(word, author, n)
head(tf_idf)
```


## Tokenisation par n_gramme

```{r}
einstein_bigrams <- books %>% 
       filter(author == "Einstein, Albert") %>% 
       unnest_tokens(bigram, text, token = "ngrams", n = 2)

head(einstein_bigrams)
```

```{r}
einstein_bigrams_count <- einstein_bigrams %>% 
       count(bigram, sort = TRUE)
einstein_bigrams_count
```

```{r}
library(tidyr)
bigrams_separeted <- einstein_bigrams %>% 
       separate(bigram, c("word1", "word2"), sep =" ")

bigrams_filtered <- bigrams_separeted %>% 
        filter(!word1 %in% stop_words$word) %>% 
        filter(!word2 %in% stop_words$word) %>% 
        filter(!is.na(word1))

bigrams_counts <- bigrams_filtered %>% 
       count(word1, word2, sort = TRUE)
bigrams_counts

```


```{r}
bigram_theory <- bigrams_filtered %>% 
       filter(word2 == "theory") %>% 
       count(word1, sort = TRUE)

bigram_theory
```

```{r}
trigram <- books %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word,  
         !is.na(word1)) %>%
  count(word1, word2, word3, sort = TRUE)

trigram
```

# Analyse du réseau

```{r}
library(igraph)
library(ggraph)
```

```{r}
bigrams_counts
```

```{r}
bigram_graph <- bigrams_counts %>% 
       filter(n > 5) %>% 
       graph_from_data_frame()
bigram_graph
```
```{r}
ggraph(bigram_graph, layout = "fr")+
       geom_edge_link()+
       geom_node_point()+
       geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

```{r}
library(grid)
a  <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
       geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                      arrow = a, end_cap = circle(.07, 'inches'))+
       geom_node_point(color = "lightblue", size = 5)+
       geom_node_text(aes(label = name), vjust = 1, hjust = 1)+
       theme_void()
```

# Classification avec regression logistic

```{r}
library(rsample)
books_split <- books %>% 
       select(document) %>% 
       initial_split(prop = 3/4)
books_split

train_data <- training(books_split)
test_data <- testing(books_split)


```


```{r}
library(tidytext)

sparse_words <- tidy_books %>% 
       count(document, word) %>% 
       inner_join(train_data, by = "document") %>% 
       cast_sparse(document, word, n)

sparse_words
```

```{r}
dim(sparse_words)
```

```{r}
word_row_names <- as.integer(rownames(sparse_words))

books_joined <- tibble(document = word_row_names) %>% 
       left_join(books %>% 
                        select(document, author))
       

```
## Modelisation de regression logistique

```{r}
library(glmnet)
library(doMC)
registerDoMC(cores = 8)

is_einstein <- books_joined$author == "Einstein, Albert"

model <- cv.glmnet(x = sparse_words,
                   y = is_einstein, family = "binomial",
                   parallel = TRUE,
                   keep = TRUE)

```

```{r}
library(broom)
coefs <- model$glmnet.fit %>% 
       tidy() %>% 
       filter(lambda == model$lambda)
coefs
```


```{r}
library(forcats)

coefs %>%
  group_by(estimate > 0) %>%
  top_n(10, abs(estimate)) %>%
  ungroup() %>%
  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  coord_flip() +
  labs(
    x = NULL,
    title = "Coefficients that increase/decrease probability the most",
    subtitle = "A document mentioning lecture or probably is unlikely to be written by Albert Einstein"
  ) +
  theme_classic(base_size = 12) +
  theme(plot.title = element_text(lineheight=.8, face="bold")) +
  scale_fill_brewer() 
```
# Evaluation du modele

```{r}
intercept <- coefs %>% 
       filter(term == "(Intercept)") %>% 
       pull(estimate)

classifications <- tidy_books %>%
  inner_join(test_data) %>%
  inner_join(coefs, by = c("word" ="term")) %>%
  group_by(document) %>%
  summarize(score = sum(estimate)) %>%
  mutate(probability = plogis(intercept + score))
      

```


```{r}
library(yardstick)

library(yardstick)

comment_classes <- classifications %>%
  left_join(books %>%
    select(author, document), by = "document") %>%
  mutate(author = as.factor(author))

comment_classes %>%
  roc_curve(author, probability) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(
    color = "midnightblue",
    size = 1.5
  ) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  ) +
  labs(
    title = "ROC curve for text classification using regularized regression",
    subtitle = "Predicting whether text was written by Albert Einstein or Nikola Tesla"
  ) +
  theme_classic(base_size = 12) +
  theme(plot.title = element_text(lineheight=.8, face="bold"))
```


```{r}
auc <- comment_classes %>%
  roc_auc(author, probability)
```

```{r}
comment_classes %>%
  mutate(prediction = case_when(
          probability > 0.5 ~ "Einstein, Albert",
          TRUE ~ "Tesla, Nikola"),
        prediction = as.factor(prediction)) %>%
  conf_mat(author, prediction)
```

```{r}
FP<- comment_classes %>%
  filter(probability > .8,
          author == "Tesla, Nikola") %>%
  sample_n(10) %>%
  inner_join(books %>%
  select(document, text)) %>%
  select(probability, text)
```

```{r}
FN <- comment_classes %>%
  filter(probability < .3,
         author == "Einstein, Albert") %>%
  sample_n(10) %>%
  inner_join(books %>%
  select(document, text)) %>%
  select(probability, text)
```



